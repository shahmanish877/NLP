{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "b"
      ],
      "metadata": {
        "id": "URY7N4kVzqjV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Stemming\n",
        "\n",
        "Stemming is the process of producing morphological variants of a root/base word. Stemming programs are commonly referred to as stemming algorithms or stemmers. A stemming algorithm reduces the words\n",
        "\n",
        "    “chocolates”, “chocolatey”, and “choco”  => root word is “chocolate”\n",
        "    “retrieval”, “retrieved”, “retrieves”  => root word is “retrieve”\n",
        "    playing, played, playfully => root word is play\n",
        "\n",
        "Here the root word formed is called ‘stem’ and it is not necessarily that stem needs to exist and have a meaning. Just by committing the suffix and prefix, we generate the stems.\n",
        "\n",
        "NLTK provides us with `PorterStemmer` `LancasterStemmer` and `SnowballStemmer` packages.\n"
      ],
      "metadata": {
        "id": "_4NyAvSyso7K"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FaDwguGkixH7"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jqYQHk3wt0QL",
        "outputId": "4d28426f-53d0-4fd3-d6b7-6b1e0d1b564b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# tokenize\n",
        "example_sent = \"This is sample sentence, showing the use of the stop word filtration\"\n",
        "stop_words = set(stopwords.words('english'))\n",
        "word_tokens = word_tokenize(example_sent,)\n",
        "\n",
        "filtered_tokens = []\n",
        "\n",
        "for w in word_tokens:\n",
        "    if w not in stop_words:\n",
        "        filtered_tokens.append(w)\n",
        "\n",
        "print(word_tokens)\n",
        "print(filtered_tokens)\n",
        "\n",
        "# alternative way\n",
        "words = [word for word in word_tokens if not word in stop_words]\n",
        "print(words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3R2y0IHJthtl",
        "outputId": "b53adcf3-5aef-45a3-e878-20b86aa6666c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['This', 'is', 'sample', 'sentence', ',', 'showing', 'the', 'use', 'of', 'the', 'stop', 'word', 'filtration']\n",
            "['This', 'sample', 'sentence', ',', 'showing', 'use', 'stop', 'word', 'filtration']\n",
            "['This', 'sample', 'sentence', ',', 'showing', 'use', 'stop', 'word', 'filtration']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Porter Stemmer\n",
        "Porter Stemmer uses suffix striping to produce stems. It does not follow the linguistic set of rules to produce stem for phases in different cases, due to this reason porter stemmer does not generate stems, i.e. actual English words."
      ],
      "metadata": {
        "id": "UBrOHbP8uJ9T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem.porter import PorterStemmer"
      ],
      "metadata": {
        "id": "BloB-_ZAuQ7M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reduce words to their stems\n",
        "stemmed = [PorterStemmer().stem(w) for w in words]\n",
        "print(stemmed)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bfl3cpN5uOGk",
        "outputId": "a1d12e9f-0114-42c5-ad22-be0aba45c3dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['thi', 'sampl', 'sentenc', ',', 'show', 'use', 'stop', 'word', 'filtrat']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### LancasterStemmer\n",
        "\n",
        "More agresive than porter"
      ],
      "metadata": {
        "id": "klnoxV2wuc27"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import LancasterStemmer\n",
        "words = ['sincerely','electricity','roughly','ringing']\n",
        "Lanc = LancasterStemmer()\n",
        "for w in words:\n",
        "    print(w, \" : \", Lanc.stem(w))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TlleUEf8ub9r",
        "outputId": "c0f96dae-7691-4ca7-f89f-75dfb5d9d040"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sincerely  :  sint\n",
            "electricity  :  elect\n",
            "roughly  :  rough\n",
            "ringing  :  ring\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SnowballStemmer\n",
        "\n",
        "Snowball Stemmer: It is a stemming algorithm which is also known as the Porter2 stemming algorithm as it is a better version of the Porter Stemmer since some issues of it were fixed in this stemmer."
      ],
      "metadata": {
        "id": "3uzUzCLsuqGb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# importing modules\n",
        "from nltk.stem import SnowballStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "snowball_stemmer = SnowballStemmer(language='english')\n",
        "\n",
        "# sentence = \"Programmers program with programming languages\"\n",
        "sentence = \"Snowball Stemmer: It is a stemming algorithm which is also known as the Porter2 stemming algorithms as it is a better version of the Porter Stemmer since some issues of it were fixed in this stemmer.\"\n",
        "words = word_tokenize(sentence)\n",
        "\n",
        "for w in words:\n",
        "    print(w, \" : \", snowball_stemmer.stem(w))"
      ],
      "metadata": {
        "id": "msUkaEAVus9r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Lemmatization:\n",
        "\n",
        "We want to extract the base form of the word here.\n",
        "\n",
        "- The word extracted here is called `Lemma` and it is available in the dictionary.\n",
        "- We have the `WordNet` corpus and the lemma generated will be available in this corpus.\n",
        "- NLTK provides us with the WordNet Lemmatizer that makes use of the WordNet Database to lookup lemmas of words.\n",
        "\n",
        "NOteLStemming is much faster than lemmatization as it doesn’t need to lookup in the dictionary and just follows the algorithm to generate the root words.\n",
        "\n",
        "Text preprocessing includes both Stemming as well as Lemmatization. __Many times people find these two terms confusing.__\n",
        "\n",
        "Some treat these two as the same. __Actually, lemmatization is preferred over Stemming because lemmatization does morphological analysis of the words__."
      ],
      "metadata": {
        "id": "6KaQ7_nwu9I7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### WordNetLemmatizer"
      ],
      "metadata": {
        "id": "9XkTYfyEyDzJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NEjLBWIevKxc",
        "outputId": "aeea6e02-fc33-416d-ae0e-bd53fdb8d7b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize"
      ],
      "metadata": {
        "id": "8CXXl3cDvMSu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wnl = WordNetLemmatizer()\n",
        "lemmed = wnl.lemmatize('fixing')\n",
        "print(lemmed)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VlmwlKW_vWIj",
        "outputId": "d84a0d99-c900-4442-996a-f693ef10bb79"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fixing\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lemmed = wnl.lemmatize('fixing', 'v')\n",
        "print(lemmed)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gB6byRUPvd4o",
        "outputId": "a3f9f5ee-0fd3-4b3a-ec4c-1d3baacd365d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fix\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Reduce words to their root form\n",
        "sentence = \"Snowball Stemmer: It is a stemming algorithm which is also known as the Porter2 stemming algorithm as it is a better version of the Porter Stemmer since some issues of it were fixed in this stemmer.\"\n",
        "words = word_tokenize(sentence)\n",
        "# print(words)\n",
        "lemmed = [WordNetLemmatizer().lemmatize(w.lower()) for w in words]\n",
        "print(lemmed)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zz8pPdk0vA6B",
        "outputId": "05760df4-e8a7-4701-9844-291f915d9a21"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['snowball', 'stemmer', ':', 'it', 'is', 'a', 'stemming', 'algorithm', 'which', 'is', 'also', 'known', 'a', 'the', 'porter2', 'stemming', 'algorithm', 'a', 'it', 'is', 'a', 'better', 'version', 'of', 'the', 'porter', 'stemmer', 'since', 'some', 'issue', 'of', 'it', 'were', 'fixed', 'in', 'this', 'stemmer', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To make wordnet lematizer work we need to pass the tag with word\n",
        "\n",
        "that can be done by analyzing sentence"
      ],
      "metadata": {
        "id": "ypjcR59nwEtj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### TextBlob"
      ],
      "metadata": {
        "id": "i-KfPHYmyAwU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install textblob"
      ],
      "metadata": {
        "id": "w0UR-nsTwYBT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "onzwKKn0wr8t",
        "outputId": "fde01804-ff05-4e61-b4ad-357834e6cf7a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from textblob import TextBlob, Word"
      ],
      "metadata": {
        "id": "1b2kScleycBs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Lemmatize a word\n",
        "word = 'stripes'\n",
        "w = Word(word)\n",
        "print(w)\n",
        "w.lemmatize()\n",
        "#> stripe"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "PJmFR42YxGzj",
        "outputId": "674e26df-b38e-488c-fa1d-0e275da95270"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "stripes\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'stripe'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import textblob\n",
        "# sentence = \"The striped bat be hang on their foot for best\"\n",
        "sentence = \"The were was supposed to be be\"\n",
        "sent = TextBlob(sentence)\n",
        "print(sent)\n",
        "print(sent.tags)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AtLRa-qxwcLr",
        "outputId": "328850e1-3325-45a0-ce57-fa2337fca366"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The were was supposed to be be\n",
            "[('The', 'DT'), ('were', 'VBD'), ('was', 'VBD'), ('supposed', 'VBN'), ('to', 'TO'), ('be', 'VB'), ('be', 'VB')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "w= Word('were')\n",
        "print(w.lemmatize('VBD'))\n",
        "print(w.lemmatize('v'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oS8qn-noy7wN",
        "outputId": "bf46c769-e322-45a8-c9ee-577bf64592ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "be\n",
            "be\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# we have to convert the pos tag into the tag name undertandable by lemitizer\n",
        "tag_dict = {\n",
        "    # \"D\": 'd',\n",
        "    \"J\": 'a',\n",
        "    \"N\": 'n',\n",
        "    \"V\": 'v',\n",
        "    \"R\": 'r'\n",
        "}\n",
        "\n",
        "words_and_tags = [(w, tag_dict.get(pos[0], 'n')) for w, pos in sent.tags]\n",
        "lemmatized_list = [wd.lemmatize(tag) for wd, tag in words_and_tags]\n",
        "print(sent.tags)\n",
        "print(words_and_tags)\n",
        "print(lemmatized_list)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oCMdcUgWynnm",
        "outputId": "1ed3a406-0348-49ff-bd5a-9bdee059082a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('The', 'DT'), ('were', 'VBD'), ('was', 'VBD'), ('supposed', 'VBN'), ('to', 'TO'), ('be', 'VB'), ('be', 'VB')]\n",
            "[('The', 'n'), ('were', 'v'), ('was', 'v'), ('supposed', 'v'), ('to', 'n'), ('be', 'v'), ('be', 'v')]\n",
            "['The', 'be', 'be', 'suppose', 'to', 'be', 'be']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# for wordnet\n",
        "wnl_lemmas = [wnl.lemmatize(wd, tag) for wd, tag in words_and_tags]\n",
        "print(wnl_lemmas)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q8OoBMmA1I19",
        "outputId": "99315529-c7ac-446d-af55-5f44aaf7ff6c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['The', 'be', 'be', 'suppose', 'to', 'be', 'be']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Other Lemitizers\n",
        "\n",
        "- Spacy Lemmatizer\n",
        "- CLiPS Pattern\n",
        "- Stanford CoreNLP\n",
        "- Gensim Lemmatizer\n",
        "- TreeTagger"
      ],
      "metadata": {
        "id": "IKCYd5aOvuO8"
      }
    }
  ]
}